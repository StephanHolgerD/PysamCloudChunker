{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71e4dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "#import streamlit as st\n",
    "#from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import subprocess\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "#from ST_varvisDownloader import varvis_api_login, get_analysisIds_per_person, get_download_link, collect_download_links\n",
    "import pysam\n",
    "from time import time\n",
    "from multiprocessing import Pool,Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "21edbf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drukewitz/miniconda3_new/envs/Jupyter/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad1229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varvis_api_login(sessionId, token, target, user_name, password):\n",
    "    \n",
    "    # 1) Get CSRF token and session ID to log in\n",
    "    r = requests.head(\"https://\" + target + \".varvis.com/authenticate\")\n",
    "    token = r.headers['X-CSRF-TOKEN']\n",
    "    sessionId = r.cookies['session']\n",
    "     \n",
    "    # 2) Log in and update CSRF token in an additional step\n",
    "    r = requests.post(\"https://\" + target + \".varvis.com/login\", data = {'_csrf': token, 'username': user_name, 'password': password}, cookies = dict(session=sessionId))\n",
    "    sessionId = r.cookies['session']\n",
    "    \n",
    "    ### THIS ADDITIONAL REQUEST IS NECESSARY TO RETRIEVE THE CORRECT CSRF TOKEN:\n",
    "    r = requests.head(\"https://\" + target + \".varvis.com/authenticate\", cookies = dict(session=sessionId))\n",
    "    token = r.headers['X-CSRF-TOKEN']\n",
    "    \n",
    "    return sessionId, token\n",
    "\n",
    "def get_analysisIds_per_person(personId, sessionId):\n",
    "    \n",
    "    r = requests.get(\"https://\" + target + \".varvis.com/person/\" + personId + \"/analyses\", cookies = dict(session=sessionId))\n",
    "\n",
    "    return json.loads(r.text)\n",
    "\n",
    "def get_download_link(analysisId, sessionId):\n",
    "    \n",
    "    r = requests.get(\"https://\" + target + \".varvis.com/api/analysis/\" + analysisId + \"/get-file-download-links\", cookies = dict(session=sessionId))\n",
    "    \n",
    "    response = json.loads(r.text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def collect_download_links(personIds, sessionId):\n",
    "    analysisIds = []\n",
    "    limsIds= []\n",
    "    sampleIds= []\n",
    "    all_fileNames= []\n",
    "    all_links= []\n",
    "    fileNames = []\n",
    "    AnType=[]\n",
    "    links = []\n",
    "    for person in personIds:\n",
    "        print(person)\n",
    "        analysis_response = get_analysisIds_per_person(person, sessionId)\n",
    "        if not type(analysis_response) == list:\n",
    "            if \"errorMessageId\" in analysis_response.keys():\n",
    "                AnType.append('personID not found')\n",
    "                analysisIds.append(\"personID not found\")\n",
    "                limsIds.append(person)\n",
    "                sampleIds.append(\"personID not found\")\n",
    "                fileNames.append(\"personID not found\")\n",
    "                links.append(\"personID not found\")\n",
    "                \n",
    "                continue # this skips the rest of the loop and continues with the next personID\n",
    "            \n",
    "        download_responses = []\n",
    "        # !!!! Download links can only be obtained for 'SNV' analyses\n",
    "        \n",
    "        for row in analysis_response:\n",
    "            analysisId =str(row[\"id\"])\n",
    "            download_response = get_download_link(analysisId, sessionId)\n",
    "            \n",
    "            if download_response['errorExpected']:\n",
    "                continue\n",
    "            apiFileLinks = download_response[\"response\"][\"apiFileLinks\"]\n",
    "            for file in apiFileLinks:\n",
    "                AnType.append(row['enrichmentKitName'])\n",
    "                analysisIds.append(download_response[\"response\"][\"id\"])\n",
    "                limsIds.append(download_response[\"response\"][\"limsId\"])\n",
    "                sampleIds.append(download_response[\"response\"][\"sampleId\"])\n",
    "                fileNames.append(file[\"fileName\"])\n",
    "                links.append(file[\"downloadLink\"])\n",
    "    df_overview = pd.DataFrame()  \n",
    "    df_overview[\"analysisId\"] = analysisIds\n",
    "    df_overview['analysisType']=AnType\n",
    "    df_overview[\"limsIds\"] = limsIds\n",
    "    df_overview[\"sampleIds\"] = sampleIds\n",
    "    df_overview[\"fileNames\"] = fileNames\n",
    "    df_overview[\"links\"] = links\n",
    "\n",
    "    return df_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec397348",
   "metadata": {},
   "outputs": [],
   "source": [
    "target='uni-leipzig'\n",
    "user_name='hugapi'\n",
    "password=\"\"\n",
    "sessionId = \"\"\n",
    "token = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../00_fullBams/Repeat/data/variant_catalog_with_offtargets.GRCh37.json') as j:\n",
    "    x=j.read()\n",
    "aa =json.loads(x)\n",
    "positions = set()\n",
    "chroms=[]\n",
    "starts=[]\n",
    "ends=[]\n",
    "\n",
    "for roi in aa:\n",
    "    RR = roi['ReferenceRegion']\n",
    "    if type(RR)==str:\n",
    "        positions.add(RR)\n",
    "    else:\n",
    "        for o in RR:\n",
    "            positions.add(o)\n",
    "    if 'OfftargetRegions' not in roi:\n",
    "        continue\n",
    "    OR = roi['OfftargetRegions']\n",
    "    for o in OR:\n",
    "        positions.add(o)\n",
    "\n",
    "        \n",
    "chroms = [x.split(':')[0].replace('chr','') for x in positions]\n",
    "starts = [int(x.split(':')[1].split('-')[0]) for x in positions]\n",
    "ends = [int(x.split(':')[1].split('-')[1]) for x in positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8aef7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetChunk(x):\n",
    "    bam = x[0]\n",
    "    bai = x[1]\n",
    "    c = x[2]\n",
    "    s = x[3]\n",
    "    e = x[4]\n",
    "    padding = x[5]\n",
    "    reads = set()\n",
    "    with pysam.AlignmentFile(bam,filepath_index=bai) as f:\n",
    "        for r in f.fetch(c,s-padding,e+padding,until_eof=True):\n",
    "            r=r.to_string()\n",
    "            if r not in reads:\n",
    "                reads.add(r)\n",
    "    return reads\n",
    "\n",
    "def GetBam(bam,bai,chrom,start,stop,outname,padding=50,proc=12):\n",
    "    mp_split = [(bam,bai,c,s,e,padding) for c,s,e in zip(chrom,start,stop)]\n",
    "    with Pool(processes=proc) as p:\n",
    "       # r = p.map(GetChunk,tqdm(mp_split,total=len(mp_split)))\n",
    "        r = list(tqdm(p.imap(GetChunk, mp_split), total=len(mp_split)))\n",
    "    rr = [xx for x in r for xx in x]    \n",
    "    outname_temp = outname.replace('.bam','unsorted.bam')\n",
    "        \n",
    "    with pysam.AlignmentFile(bam,filepath_index=bai) as f:\n",
    "        with pysam.AlignmentFile(outname_temp,'wb',template=f) as bamout:\n",
    "            for r in rr:\n",
    "                bamout.write(pysam.AlignedSegment.fromstring(r,f.header))\n",
    "                \n",
    "    pysam.sort(\"-o\", outname, outname_temp)\n",
    "    pysam.index(outname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c686dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029421\n",
      "2029421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 169/169 [00:02<00:00, 60.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.506390333175659\n",
      "2029546\n",
      "2029546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 169/169 [00:03<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.61121916770935\n",
      "2030152\n",
      "2030152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 169/169 [00:03<00:00, 45.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.784922122955322\n",
      "2030181\n",
      "2030181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 169/169 [00:03<00:00, 44.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.048022985458374\n"
     ]
    }
   ],
   "source": [
    "with open('../../00_fullBams/Repeat/bams.txt') as b:\n",
    "    sessionId, token = varvis_api_login(sessionId, token, target, user_name, password)\n",
    "    for bam in b:\n",
    "        x=time()\n",
    "        i = bam.split('/')[-1].split('_')[0]\n",
    "        print(i)\n",
    "        d = collect_download_links([i],sessionId)\n",
    "        d_bam = d[(d['analysisType'].str.contains('Exome')) & (d['fileNames'].str.endswith('.bam'))]\n",
    "        d_bai = d[(d['analysisType'].str.contains('Exome')) & (d['fileNames'].str.endswith('.bai'))]\n",
    "        for n,(bam,bai) in enumerate(zip(d_bam['links'],d_bai['links'])):\n",
    "            with pysam.AlignmentFile(bam,filepath_index=bai) as f:\n",
    "                pass\n",
    "            GetBam(bam,bai,chroms,starts,ends,f'{n}_{i}_varvis.bam',padding=600, proc=100)\n",
    "        print(time()-x)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
